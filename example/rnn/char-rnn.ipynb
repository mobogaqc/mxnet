{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import imp\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Glance of LSTM structure and embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://webdocs.cs.ualberta.ca/~bx3/char-rnn_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://webdocs.cs.ualberta.ca/~bx3/char-rnn_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lstm import lstm_unroll, lstm_inference_symbol\n",
    "from bucket_io import BucketSentenceIter\n",
    "from rnn_model import LSTMInferenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from doc\n",
    "def read_content(path):\n",
    "    with open(path) as ins:\n",
    "        content = ins.read()\n",
    "        return content\n",
    "\n",
    "def build_vocab(path):\n",
    "    content = read_content(path)\n",
    "    content = list(content)\n",
    "    idx = 1 # 0 is left for zero-padding\n",
    "    the_vocab = {}\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if not word in the_vocab:\n",
    "            the_vocab[word] = idx\n",
    "            idx += 1\n",
    "    return the_vocab\n",
    "\n",
    "def text2id(sentence, the_vocab):\n",
    "    words = list(sentence)\n",
    "    words = [the_vocab[w] for w in words if len(w) > 0]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "def Perplexity(label, pred):\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buckets = [129]\n",
    "num_hidden = 512\n",
    "num_embed = 256\n",
    "num_lstm_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devs = [mx.context.gpu(i) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(\"./obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate symbol\n",
    "def sym_gen(seq_len):\n",
    "    return lstm_unroll(num_lstm_layer, seq_len, len(vocab) + 1,\n",
    "                       num_hidden=num_hidden, num_embed=num_embed,\n",
    "                       num_label=len(vocab) + 1, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initalize states for LSTM\n",
    "init_c = [('l%d_init_c'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_h = [('l%d_init_h'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_states = init_c + init_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset ==================\n",
      "bucket of len 129 : 8290 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bing/mxnet/example/rnn_new/bucket_io.py:153: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  bucket_plan = np.hstack([np.zeros(n, int)+i for i, n in enumerate(bucket_n_batches)])\n"
     ]
    }
   ],
   "source": [
    "data_train = BucketSentenceIter(\"./obama.txt\", vocab, buckets, batch_size,\n",
    "                                init_states, seperate_char='\\n',\n",
    "                                text2id=text2id, read_content=read_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol = sym_gen(buckets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(ctx=devs,\n",
    "                             symbol=symbol,\n",
    "                             num_epoch=num_epoch,\n",
    "                             learning_rate=learning_rate,\n",
    "                             momentum=momentum,\n",
    "                             wd=0.0001,\n",
    "                             initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:10:19 INFO:Start training with [gpu(0)]\n",
      "09:10:39 INFO:Epoch[0] Batch [50]\tSpeed: 88.50 samples/sec\tTrain-Perplexity=31.854990\n",
      "09:10:57 INFO:Epoch[0] Batch [100]\tSpeed: 86.90 samples/sec\tTrain-Perplexity=27.432798\n",
      "09:11:15 INFO:Epoch[0] Batch [150]\tSpeed: 86.80 samples/sec\tTrain-Perplexity=25.817704\n",
      "09:11:34 INFO:Epoch[0] Batch [200]\tSpeed: 86.90 samples/sec\tTrain-Perplexity=24.971366\n",
      "09:11:52 INFO:Epoch[0] Batch [250]\tSpeed: 86.85 samples/sec\tTrain-Perplexity=24.455250\n",
      "09:11:56 INFO:Epoch[0] Resetting Data Iterator\n",
      "09:11:56 INFO:Epoch[0] Train-Perplexity=24.386388\n",
      "09:11:56 INFO:Epoch[0] Time cost=95.259\n",
      "09:11:56 INFO:Saved checkpoint to \"obama-0001.params\"\n",
      "09:12:14 INFO:Epoch[1] Batch [50]\tSpeed: 88.60 samples/sec\tTrain-Perplexity=22.404177\n",
      "09:12:33 INFO:Epoch[1] Batch [100]\tSpeed: 86.79 samples/sec\tTrain-Perplexity=22.363506\n",
      "09:12:51 INFO:Epoch[1] Batch [150]\tSpeed: 86.87 samples/sec\tTrain-Perplexity=22.321544\n",
      "09:13:09 INFO:Epoch[1] Batch [200]\tSpeed: 86.84 samples/sec\tTrain-Perplexity=22.280181\n",
      "09:13:28 INFO:Epoch[1] Batch [250]\tSpeed: 86.84 samples/sec\tTrain-Perplexity=22.096338\n",
      "09:13:31 INFO:Epoch[1] Resetting Data Iterator\n",
      "09:13:31 INFO:Epoch[1] Train-Perplexity=22.046661\n",
      "09:13:31 INFO:Epoch[1] Time cost=95.230\n",
      "09:13:31 INFO:Saved checkpoint to \"obama-0002.params\"\n"
     ]
    }
   ],
   "source": [
    "model.fit(X=data_train,\n",
    "          eval_metric = mx.metric.np(Perplexity),\n",
    "          batch_end_callback=mx.callback.Speedometer(batch_size, 50),\n",
    "          epoch_end_callback=mx.callback.do_checkpoint(\"obama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper strcuture for prediction\n",
    "def MakeRevertVocab(vocab):\n",
    "    dic = {}\n",
    "    for k, v in vocab.items():\n",
    "        dic[v] = k\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make input from char\n",
    "def MakeInput(char, vocab, arr):\n",
    "    idx = vocab[char]\n",
    "    tmp = np.zeros((1,))\n",
    "    tmp[0] = idx\n",
    "    arr[:] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for random sample \n",
    "def _cdf(weights):\n",
    "    total = sum(weights)\n",
    "    result = []\n",
    "    cumsum = 0\n",
    "    for w in weights:\n",
    "        cumsum += w\n",
    "        result.append(cumsum / total)\n",
    "    return result\n",
    "\n",
    "def _choice(population, weights):\n",
    "    assert len(population) == len(weights)\n",
    "    cdf_vals = _cdf(weights)\n",
    "    x = random.random()\n",
    "    idx = bisect.bisect(cdf_vals, x)\n",
    "    return population[idx]\n",
    "\n",
    "def MakeOutput(prob, vocab, sample=False, temperature=1.):\n",
    "    if sample == False:\n",
    "        idx = np.argmax(prob, axis=1)[0]\n",
    "    else:\n",
    "        fix_dict = [\"\"] + [vocab[i] for i in range(1, len(vocab) + 1)]\n",
    "        scale_prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
    "        rescale = np.exp(np.log(scale_prob) / temperature)\n",
    "        rescale[:] /= rescale.sum()\n",
    "        return _choice(fix_dict, rescale[0, :])\n",
    "    try:\n",
    "        char = vocab[idx]\n",
    "    except:\n",
    "        char = ''\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load from check-point\n",
    "_, arg_params, __ = mx.model.load_checkpoint(\"obama\", 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LSTMInferenceModel(num_lstm_layer, len(vocab) + 1,\n",
    "                           num_hidden=num_hidden, num_embed=num_embed,\n",
    "                           num_label=len(vocab) + 1, arg_params=arg_params, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a sequence\n",
    "seq_length = 600\n",
    "input_ndarray = mx.nd.zeros((1,))\n",
    "revert_vocab = MakeRevertVocab(vocab)\n",
    "output ='T'\n",
    "MakeInput(output[-1], vocab, input_ndarray)\n",
    "for i in range(seq_length):\n",
    "    prob = model.forward(input_ndarray, i==0)\n",
    "    next_char = MakeOutput(prob, revert_vocab, False)\n",
    "    output += next_char\n",
    "    MakeInput(output[-1], vocab, input_ndarray)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The American people have been talking about how to see what we can afford - to give them the chance to start by the conversation that it was the moment when they come home. It's time to start by ally reduced to the oress of our hardiners of oure scandals and his debate about what we can do to change the world as the oned and the onection wo ha the hardineres of tich their school and communism and anounces in the ones and the oure shoression. I will be able to go to college. They say they are going to be a part of the oure as about th\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
