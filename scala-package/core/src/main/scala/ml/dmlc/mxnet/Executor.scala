package ml.dmlc.mxnet

import ml.dmlc.mxnet.Base._

import scala.collection.mutable.ArrayBuffer

object Executor {
  // Get the dictionary given name and ndarray pairs.
  private def getDict(names: Array[String], ndarrays: Array[NDArray]): Map[String, NDArray] = {
    require(names.toSet.size == names.length, "Duplicate names detected")
    (names zip ndarrays).toMap
  }
}

/**
 * Symbolic Executor component of MXNet
 * @author Yizhi Liu
 *
 * Constructor: used Symbol.bind and Symbol.simple_bind instead.
 * @param handle ExecutorHandle generated by calling Bind
 * @param symbol
 * @see Symbol.bind : to create executor
 */
class Executor(val handle: ExecutorHandle, val symbol: Symbol) {
  var argArrays: Array = _
  var gradArrays: Array = _
  var auxArrays: Array = _
  var outputs: Array[NDArray] = getOutputs
  var argDict: Map[String, NDArray] = _
  /*
  self._grad_dict = None
  self._aux_dict = None
  self._monitor_callback = None
  */

  override def finalize(): Unit = {
    checkCall(_LIB.mxExecutorFree(handle))
  }

  /**
   * list all the output ndarray
   * @return A list of ndarray binded to the heads of executor.
   */
  private def getOutputs: Array[NDArray] = {
    val ndHandles = ArrayBuffer[NDArrayHandle]()
    checkCall(_LIB.mxExecutorOutputs(handle, ndHandles))
    ndHandles.toArray.map(new NDArray(_))
  }

  /**
   * Calculate the outputs specified by the binded symbol.
   * @param isTrain whether this forward is for evaluation purpose.
   * @param kwargs Additional specification of input arguments.
   */
  def forward(isTrain: Boolean, kwargs: (String, NDArray)*): Unit = {
    kwargs.foreach { case (name, array) =>
      require(argDict.contains(name), s"Unknown argument $name")
      array.copyTo(argDict(name))
    }
    checkCall(_LIB.mxExecutorForward(handle, if (isTrain) 1 else 0))
  }

  def forward(): Unit = {
    forward(isTrain = false)
  }

  /**
   * Do backward pass to get the gradient of arguments.
   * @param outGrads
   *   Gradient on the outputs to be propagated back.
   *   This parameter is only needed when bind is called
   *   on outputs that are not a loss function.
   */
  def backward(outGrads: Array[NDArray]):Unit = {
  }
}
