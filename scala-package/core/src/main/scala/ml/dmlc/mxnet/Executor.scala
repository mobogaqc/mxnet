package ml.dmlc.mxnet

import ml.dmlc.mxnet.Base._
import org.slf4j.{Logger, LoggerFactory}

import scala.collection.mutable.ArrayBuffer

object Executor {
  // Get the dictionary given name and ndarray pairs.
  private[mxnet] def getDict(names: Seq[String],
                             ndarrays: Seq[NDArray]): Map[String, NDArray] = {
    require(names.toSet.size == names.length, "Duplicate names detected")
    (names zip ndarrays).toMap
  }

  /**
   * Get input slice from the input shape.
   * @param batchSize The number of samples in a mini-batch.
   * @param workLoadList The list of work load for different devices, in the same order as ctx
   * @return The split slices to get a specific slice.
   * @throws IllegalArgumentException
   *         If there are two many splits such that some slice can be empty.
   */
  private[mxnet] def splitInputSlice(batchSize: Int,
                                     workLoadList: Seq[Float]): Array[(Int, Int)] = {
    val totalWorkLoad = workLoadList.sum
    val batchNumList = workLoadList.map(workLoad =>
      math.round(workLoad * batchSize / totalWorkLoad)).toArray
    val batchNumSum = batchNumList.sum
    if (batchNumSum < batchSize) {
      batchNumList(batchNumList.length-1) += batchSize - batchNumSum
    }

    val slices = ArrayBuffer.empty[(Int, Int)]
    var end = 0
    batchNumList.foreach(batchNum => {
      val begin = math.min(end, batchSize)
      end = math.min(begin + batchNum, batchSize)
      require(begin < end, "Too many slices such that some splits are empty")
      slices.append((begin, end))
    })
    slices.toArray
  }

  /**
   * Check the argument names of symbol.
   * This function checks the duplication of arguments in Symbol.
   * The check is done for feedforward net for now.
   * @param symbol The network configuration
   */
  private[mxnet] def checkArguments(symbol: Symbol): Unit = {
    val argNames = symbol.listArguments()
    require(argNames.toSet.size == argNames.length,
      "Find duplicated argument name," +
      "please make the weight name non-duplicated(using name arguments)," +
      s"arguments are $argNames")

    val auxNames = symbol.listAuxiliaryStates()
    require(auxNames.toSet.size == auxNames.length,
      "Find duplicated auxiliary param name," +
      "please make the weight name non-duplicated(using name arguments)," +
      s"arguments are $auxNames")
  }

  // Load a list of arrays into a list of arrays
  private[mxnet] def loadGeneral(data: Array[NDArray], targets: Array[NDArray]): Unit = {
    (data zip targets).foreach { case (dSrc, dTarget) =>
      dSrc.copyTo(dTarget)
    }
  }

  // Load a list of arrays into a list of arrays specified by slices
  private[mxnet] def loadGeneral(data: Array[NDArray],
                                 targets: Array[(Int, Int, NDArray)]): Unit = {
    (data zip targets).foreach { case (dSrc, (start, end, dTarget)) =>
      dSrc.slice(start, end).copyTo(dTarget)
    }
  }
}

/**
 * Symbolic Executor component of MXNet
 * @author Yizhi Liu
 *
 * Constructor: used Symbol.bind and Symbol.simple_bind instead.
 * @param handle ExecutorHandle generated by calling Bind
 * @param symbol
 * @see Symbol.bind : to create executor
 */
// scalastyle:off finalize
class Executor(private[mxnet] val handle: ExecutorHandle, private[mxnet] val symbol: Symbol) {
  private[mxnet] var argArrays: Array[NDArray] = null
  private[mxnet] var gradArrays: Array[NDArray] = null
  private[mxnet] var auxArrays: Array[NDArray] = null
  protected var outputs: Array[NDArray] = getOutputs
  protected var _argDict: Map[String, NDArray] = null
  protected var _auxDict: Map[String, NDArray] = null
  protected var monitorCallback: MXMonitorCallback = null

  override def finalize(): Unit = {
    checkCall(_LIB.mxExecutorFree(handle))
  }

  /**
   * list all the output ndarray
   * @return A list of ndarray binded to the heads of executor.
   */
  private def getOutputs: Array[NDArray] = {
    val ndHandles = ArrayBuffer[NDArrayHandle]()
    checkCall(_LIB.mxExecutorOutputs(handle, ndHandles))
    ndHandles.toArray.map(new NDArray(_))
  }

  /**
   * Calculate the outputs specified by the binded symbol.
   * @param isTrain whether this forward is for evaluation purpose.
   * @param kwargs Additional specification of input arguments.
   */
  def forward(isTrain: Boolean, kwargs: (String, NDArray)*): Unit = {
    kwargs.foreach { case (name, array) =>
      require(argDict.contains(name), s"Unknown argument $name")
      array.copyTo(argDict(name))
    }
    checkCall(_LIB.mxExecutorForward(handle, if (isTrain) 1 else 0))
  }

  def forward(): Unit = {
    forward(isTrain = false)
  }

  /**
   * Do backward pass to get the gradient of arguments.
   * @param outGrads
   *   Gradient on the outputs to be propagated back.
   *   This parameter is only needed when bind is called
   *   on outputs that are not a loss function.
   */
  def backward(outGrads: Array[NDArray]): Unit = {
    require(outGrads != null)
    val ndArrayPtrs = outGrads.map(_.handle)
    checkCall(_LIB.mxExecutorBackward(handle, ndArrayPtrs))
  }

  def backward(outGrad: NDArray): Unit = {
    require(outGrad != null)
    backward(Array(outGrad))
  }

  def backward(): Unit = {
    backward(Array.empty[NDArray])
  }

  /**
   * Install callback.
   * @param callback Takes a string and an NDArrayHandle.
   */
  def setMonitorCallback(callback: MXMonitorCallback): Unit = {
    monitorCallback = callback
    checkCall(_LIB.mxExecutorSetMonitorCallback(handle, monitorCallback))
  }

  /**
   * Get dictionary representation of argument arrrays.
   * @return The dictionary that maps name of arguments to NDArrays.
   * @throws IllegalArgumentException if there are duplicated names in the arguments.
   */
  def argDict: Map[String, NDArray] = {
    if (_argDict == null) {
      _argDict = Executor.getDict(symbol.listArguments(), argArrays)
    }
    _argDict
  }

  /**
   * Get dictionary representation of auxiliary states arrays.
   * @return The dictionary that maps name of auxiliary states to NDArrays.
   * @throws IllegalArgumentException if there are duplicated names in the auxiliary states.
   */
  def auxDict: Map[String, NDArray] = {
    if (_auxDict == null) {
      _auxDict = Executor.getDict(
      symbol.listAuxiliaryStates(), auxArrays)
    }
    _auxDict
  }

  /**
   * Copy parameters from arg_params, aux_params into executor's internal array.
   * @param argParams : dict of name to NDArray of arguments
   * @param auxParams : dict of name to NDArray of auxiliary states.
   * @param allowExtraParams
   *        Whether allow extra parameters that are not needed by symbol
   *        If this is True, no error will be thrown when arg_params or aux_params
   *        contain extra parameters that is not needed by the executor.
   * @throws IllegalArgumentException
   *         If there is additional parameters in the dict but allow_extra_params=False
   */
  def copyParamsFrom(argParams: Map[String, NDArray],
                     auxParams: Map[String, NDArray],
                     allowExtraParams: Boolean = false): Unit = {
    argParams.foreach { case (name, array) =>
      if (argDict.contains(name)) {
        array.copyTo(argDict(name))
      } else {
        require(allowExtraParams, s"Find name $name that is not in the arguments")
      }
    }
    if (auxParams != null) {
      auxParams.foreach { case (name, array) =>
        if (auxDict.contains(name)) {
          array.copyTo(auxDict(name))
        } else {
          require(allowExtraParams, s"Find name $name that is not in the auxiliary states")
        }
      }
    }
  }

  def copyParamsFrom(argParams: Map[String, NDArray], allowExtraParams: Boolean): Unit = {
    copyParamsFrom(argParams, null, allowExtraParams)
  }

  def copyParamsFrom(argParams: Map[String, NDArray]): Unit = {
    copyParamsFrom(argParams, allowExtraParams = false)
  }

  /**
   * Get a debug string about internal execution plan.
   * @return Debug string of the executor.
   */
  def debugStr: String = {
    val str = new RefString
    checkCall(_LIB.mxExecutorPrint(handle, str))
    str.value
  }
}
// scalastyle:on finalize

/**
 * Helper class to manage multiple executors for data parallelism.
 * @param symbol output symbol
 * @param ctx devices to run on
 * @param paramNames Name of all trainable parameters of the network.
 * @param argNames Name of all arguments of the network.
 * @param auxNames Name of all auxiliary states of the network.
 * @param trainData Training data iterator.
 * @param workLoadList The list of work load for different devices, in the same order as ctx
 * @param logger When not specified, default logger will be used.
 */
class DataParallelExecutorManager(symbol: Symbol,
                                  ctx: Array[Context],
                                  paramNames: Seq[String],
                                  argNames: Seq[String],
                                  auxNames: Seq[String],
                                  trainData: DataIter,
                                  private var workLoadList: Seq[Float] = null,
                                  logger: Logger = DataParallelExecutorManager.logger) {
  // preparation
  val numDevice = ctx.length
  logger.info(s"Start training with ${ctx.mkString(",")}")

  // make sure the architecture is valid
  Executor.checkArguments(symbol)

  if (workLoadList == null) {
    workLoadList = List.fill(numDevice)(1f)
  }
  require(workLoadList.size == numDevice, "Invalid settings for work load.")

  val slices = Executor.splitInputSlice(trainData.batchSize, workLoadList)

  /*
  self.train_execs = []
  for i in range(len(ctx)):
    data_shapes = {k: tuple([slices[i].stop-slices[i].start] + list(v[1:]))
      for k, v in train_data.provide_data}
    train_exec = symbol.simple_bind(ctx[i], 'write', **data_shapes)
    self.train_execs.append(train_exec)

  // data structure
  self.data_names = [x[0] for x in train_data.provide_data]
  self.label_names = [x[0] for x in train_data.provide_label]
  self.aux_names = aux_names

  self.data_arrays = [[(slices[i], e.arg_dict[name]) for i, e in enumerate(self.train_execs)]
  for name in self.data_names]
  self.label_arrays = [[(slices[i], e.arg_dict[name]) for i, e in enumerate(self.train_execs)]
  for name in self.label_names]

  self.param_idx = [i for i in range(len(arg_names)) if arg_names[i] in param_names]
  self.param_names = [arg_names[i] for i in self.param_idx]
  self.param_arrays = [[e.arg_arrays[i] for e in self.train_execs] for i in self.param_idx]
  self.grad_arrays = [[e.grad_arrays[i] for e in self.train_execs] for i in self.param_idx]

  self.aux_arrays = [[e.aux_arrays[i] for e in self.train_execs] for i in range(len(aux_names))]

  batch_size = train_data.batch_size

  output_shapes = [tuple([batch_size]+list(x.shape[1:])) for x in self.train_execs[0].outputs]
  self.cpu_output_arrays = [nd.zeros(s) for s in output_shapes]
  */
}

object DataParallelExecutorManager {
  private val logger = LoggerFactory.getLogger(classOf[Model])
}

